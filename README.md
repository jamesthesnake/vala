The current reward structure (0.3 for correctness + speedup) treats all optimizations equally, which can lead to suboptimal learning dynamics. Progressive reward scheduling addresses a fundamental challenge in code optimization: you can't optimize code that doesn't work. By starting with correctness-heavy rewards (e.g., 0.7 correctness, 0.3 performance) and gradually shifting weight toward performance (0.3 correctness, 0.7 performance), the model first masters generating valid CUDA kernels before learning advanced optimizations. Logarithmic speedup rewards (e.g., log(1 + speedup)) prevent the model from overfitting to "easy wins" where naive optimizations yield 10x speedups, encouraging it to tackle harder problems where 1.5x speedup requires sophisticated techniques. Adding penalties for compilation warnings catches subtle issues that might cause problems on different hardware or edge cases. Memory efficiency metrics are crucial because real-world CUDA performance isn't just about speed—a kernel that's 20% faster but uses 3x more memory might cause OOM errors in production or force smaller batch sizes, negating its benefits.
2. Smarter Trajectory Management
Fixed refinement steps waste computational resources and can reinforce bad habits. When a kernel plateaus at 1.2x speedup after 3 attempts, forcing 5 more refinements teaches the model to generate meaningless variations rather than recognizing optimization limits. Adaptive stopping based on performance plateaus (no improvement for 2 steps) helps the model learn when to stop trying, which is valuable meta-knowledge. Early termination for high-performing kernels (>2x speedup) allows more compute to be allocated to challenging problems—if you've achieved excellent performance, those compute cycles are better spent on harder tasks. Dynamic step allocation based on task difficulty acknowledges that matrix multiplication might need 10 refinement steps to explore tiling strategies, shared memory usage, and tensor core utilization, while a simple element-wise operation might be optimized in 2-3 steps. This approach also provides implicit curriculum learning, as the model learns to gauge problem difficulty and allocate effort accordingly.
3. Curriculum Learning
Starting with mixed difficulties can overwhelm the model and create unstable training dynamics, similar to teaching calculus before algebra. Beginning with Level 1 tasks establishes foundational CUDA concepts: thread indexing, memory coalescing, basic shared memory usage, and warp primitives. Only after achieving 70%+ success rate on these fundamentals should Level 2's fused kernels be introduced, as they require combining multiple optimization techniques coherently. Creating synthetic intermediate tasks bridges the gap—for example, between "simple matrix multiply" and "fused attention," you might create "matrix multiply with softmax" or "matrix multiply with custom reduction." Ordering tasks within levels by complexity (measured by number of memory accesses, arithmetic intensity, or required optimization techniques) creates a smooth learning curve. This approach mirrors how human programmers learn CUDA: starting with vector addition, progressing to matrix operations, then tackling complex fused kernels. The curriculum also helps with credit assignment—when the model successfully optimizes a complex kernel, it can attribute success to specific techniques learned on simpler problems.RetryClaude can make mistakes. Please double-check responses.
